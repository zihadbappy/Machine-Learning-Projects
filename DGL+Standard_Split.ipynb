{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zihadbappy/Machine-Learning-Projects/blob/master/DGL%2BStandard_Split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "from torchvision import datasets, transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "import math\n",
        "import datetime\n",
        "import itertools\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# ==========================================\n",
        "# 1. Utility Functions\n",
        "# ==========================================\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "# ==========================================\n",
        "# 2. Model Definitions (Unchanged)\n",
        "# ==========================================\n",
        "\n",
        "class identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(identity, self).__init__()\n",
        "    def forward(self, input):\n",
        "        return input\n",
        "\n",
        "class DownsampleA(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(DownsampleA, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "class ResNetBasicblock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(ResNetBasicblock, self).__init__()\n",
        "        self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn_a = nn.BatchNorm2d(planes)\n",
        "        self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_b = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        basicblock = self.conv_a(x)\n",
        "        basicblock = self.bn_a(basicblock)\n",
        "        basicblock = F.relu(basicblock, inplace=True)\n",
        "        basicblock = self.conv_b(basicblock)\n",
        "        basicblock = self.bn_b(basicblock)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        return F.relu(residual + basicblock, inplace=True)\n",
        "\n",
        "class CifarResNet(object):\n",
        "    def __init__(self, block, depth, num_classes):\n",
        "        super(CifarResNet, self).__init__()\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        self.num_classes = num_classes\n",
        "        self.layers = []\n",
        "        self.conv_1_3x3 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layers.append(self.conv_1_3x3)\n",
        "        self.bn_1 = nn.BatchNorm2d(16)\n",
        "        self.layers.append(self.bn_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layers.append(self.relu)\n",
        "        list_planes = [16,] * layer_blocks + [32,] * layer_blocks + [64,] * layer_blocks\n",
        "        list_stride = [1, 2, 2]\n",
        "        self.inplanes = 16\n",
        "        for i, planes in enumerate(list_planes):\n",
        "            stride = 1\n",
        "            downsample = None\n",
        "            if i % layer_blocks == 0:\n",
        "                stride = list_stride[i // layer_blocks]\n",
        "                if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "                    downsample = DownsampleA(self.inplanes, planes * block.expansion, stride)\n",
        "            self.layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "            if i % layer_blocks == 0:\n",
        "                self.inplanes = planes * block.expansion\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.layers.append(self.avgpool)\n",
        "        self.classifier = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "class CifarResNetDDG(nn.Module):\n",
        "    def __init__(self, model, layers, splits_id, num_splits, delay):\n",
        "        super(CifarResNetDDG, self).__init__()\n",
        "        self.splits_id = splits_id\n",
        "        self.num_splits = num_splits\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None: m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        if self.splits_id == self.num_splits - 1:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class auxillary_classifier2(nn.Module):\n",
        "    def __init__(self, feature_size=256, input_features=256, in_size=32,\n",
        "                 num_classes=10, n_lin=0, mlp_layers=0, batchn=True):\n",
        "        super(auxillary_classifier2, self).__init__()\n",
        "        self.n_lin = n_lin\n",
        "        self.in_size = in_size\n",
        "        self.mlp = mlp_layers > 0\n",
        "        if n_lin == 0: feature_size = input_features\n",
        "        current_input_features = input_features\n",
        "        self.blocks = []\n",
        "        for n in range(self.n_lin):\n",
        "            if n == 0: in_f = current_input_features\n",
        "            else: in_f = feature_size\n",
        "            bn_temp = nn.BatchNorm2d(feature_size) if batchn else identity()\n",
        "            conv = nn.Conv2d(in_f, feature_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "            self.blocks.append(nn.Sequential(conv, bn_temp))\n",
        "        self.blocks = nn.ModuleList(self.blocks)\n",
        "        self.bn = nn.BatchNorm2d(feature_size) if batchn else identity()\n",
        "\n",
        "        if mlp_layers > 0:\n",
        "            mlp_feat = feature_size * 2 * 2\n",
        "            layers = []\n",
        "            for l in range(mlp_layers):\n",
        "                if l == 0: in_feat = feature_size * 4\n",
        "                else: in_feat = mlp_feat\n",
        "                bn_temp = nn.BatchNorm1d(mlp_feat) if batchn else identity()\n",
        "                layers += [nn.Linear(in_feat, mlp_feat), bn_temp, nn.ReLU(True)]\n",
        "            layers += [nn.Linear(mlp_feat, num_classes)]\n",
        "            self.classifier = nn.Sequential(*layers)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(feature_size * 2 * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        out = F.adaptive_avg_pool2d(out, (math.ceil(self.in_size / 4), math.ceil(self.in_size / 4)))\n",
        "        for n in range(self.n_lin):\n",
        "            out = self.blocks[n](out)\n",
        "            out = F.relu(out)\n",
        "        out = F.adaptive_avg_pool2d(out, (2, 2))\n",
        "        if not self.mlp: out = self.bn(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "class rep(nn.Module):\n",
        "    def __init__(self, blocks):\n",
        "        super(rep, self).__init__()\n",
        "        self.blocks = blocks\n",
        "    def forward(self, x, n, upto=False):\n",
        "        if upto:\n",
        "            for i in range(n + 1):\n",
        "                x = self.forward(x, i, upto=False)\n",
        "            return x\n",
        "        out = self.blocks[n](x)\n",
        "        return out\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, depth=110, num_classes=10, num_splits=2):\n",
        "        super(Net, self).__init__()\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        self.auxillary_nets = nn.ModuleList([])\n",
        "        model = CifarResNet(ResNetBasicblock, depth, num_classes)\n",
        "        len_layers = len(model.layers)\n",
        "        split_depth = math.ceil(len_layers / num_splits)\n",
        "\n",
        "        for splits_id in range(num_splits):\n",
        "            left_idx = splits_id * split_depth\n",
        "            right_idx = (splits_id + 1) * split_depth\n",
        "            if right_idx > len_layers: right_idx = len_layers\n",
        "\n",
        "            # Create Split Block\n",
        "            net = CifarResNetDDG(model, model.layers[left_idx:right_idx], splits_id, num_splits, num_splits - 1 - splits_id)\n",
        "            self.blocks.append(net)\n",
        "\n",
        "            # Create Aux Net or Final Classifier\n",
        "            if splits_id < num_splits - 1:\n",
        "                self.auxillary_nets.append(\n",
        "                    auxillary_classifier2(input_features=32, in_size=16, num_classes=num_classes, n_lin=3, mlp_layers=3))\n",
        "            else:\n",
        "                self.auxillary_nets.append(model.classifier)\n",
        "\n",
        "        self.main_cnn = rep(self.blocks)\n",
        "\n",
        "    def forward(self, representation, n, upto=False):\n",
        "        representation = self.main_cnn.forward(representation, n, upto=upto)\n",
        "        outputs = self.auxillary_nets[n](representation)\n",
        "        return outputs, representation\n",
        "\n",
        "# ==========================================\n",
        "# 3. Training & Validation Logic\n",
        "# ==========================================\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch, device, mode):\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    model.eval()\n",
        "    last_split_idx = len(model.main_cnn.blocks) - 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            input = input.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "            outputs, _ = model(input, n=last_split_idx, upto=True)\n",
        "            loss = criterion(outputs, target)\n",
        "            prec1 = accuracy(outputs, target)\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1[0].item(), input.size(0))\n",
        "\n",
        "    return top1.avg, losses.avg\n",
        "\n",
        "def train_dgl_bp_free(train_loader, model, criterion, optimizers, device):\n",
        "    model.train()\n",
        "    ncnn = len(model.main_cnn.blocks)\n",
        "    losses = [AverageMeter() for _ in range(ncnn)]\n",
        "    top1 = [AverageMeter() for _ in range(ncnn)]\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        representation = inputs\n",
        "\n",
        "        for n in range(ncnn):\n",
        "            optimizers[n].zero_grad()\n",
        "            outputs, representation = model(representation, n=n)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizers[n].step()\n",
        "            representation = representation.detach()\n",
        "\n",
        "            prec1 = accuracy(outputs, targets)\n",
        "            losses[n].update(loss.item(), inputs.size(0))\n",
        "            top1[n].update(prec1[0].item(), inputs.size(0))\n",
        "\n",
        "    return losses[-1].avg, top1[-1].avg\n",
        "\n",
        "def train_standard_split(train_loader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    ncnn = len(model.main_cnn.blocks)\n",
        "    last_split_idx = ncnn - 1\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        representation = inputs\n",
        "\n",
        "        for n in range(ncnn):\n",
        "            if n == last_split_idx:\n",
        "                # The final call uses Net.forward(), which returns (outputs, representation)\n",
        "                outputs, representation = model(representation, n=n)\n",
        "            else:\n",
        "                # Intermediate calls use rep.forward(), which returns ONLY representation\n",
        "                # FIX: Removed \"_, \" unpacking here\n",
        "                representation = model.main_cnn(representation, n=n)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        prec1 = accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        top1.update(prec1[0].item(), inputs.size(0))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "# ==========================================\n",
        "# 4. Main Execution\n",
        "# ==========================================\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR-10 DGL vs Standard Split Learning')\n",
        "parser.add_argument('--batch-size', type=int, default=256, metavar='N', help='Input batch size')\n",
        "parser.add_argument('--test-batch-size', type=int, default=256, metavar='N', help='Test batch size')\n",
        "parser.add_argument('--epochs', type=int, default=40, metavar='N', help='Number of epochs')\n",
        "parser.add_argument('--lr', type=float, default=0.1, metavar='LR', help='Learning rate')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='Random seed')\n",
        "\n",
        "def main():\n",
        "    # Pass empty list for notebooks\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Reproducibility\n",
        "    torch.manual_seed(args.seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # Data\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=use_cuda)\n",
        "    testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    val_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=4, pin_memory=use_cuda)\n",
        "\n",
        "    # MODES TO RUN\n",
        "    modes = ['dgl', 'standard']\n",
        "    results = {}\n",
        "\n",
        "    for mode in modes:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\" STARTING MODE: {mode.upper()} \")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Reset Model and Optimizer for each mode\n",
        "        model = Net(depth=110, num_classes=10, num_splits=2).to(device)\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "        ncnn = len(model.main_cnn.blocks)\n",
        "\n",
        "        # Setup Optimizers\n",
        "        optimizers = []\n",
        "        optimizer = None\n",
        "\n",
        "        if mode == 'dgl':\n",
        "            for n in range(ncnn):\n",
        "                params = itertools.chain(model.main_cnn.blocks[n].parameters(), model.auxillary_nets[n].parameters())\n",
        "                optimizers.append(optim.SGD(params, lr=args.lr, momentum=0.9, weight_decay=5e-4))\n",
        "        else:\n",
        "            params = list(model.main_cnn.parameters()) + list(model.auxillary_nets[-1].parameters())\n",
        "            optimizer = optim.SGD(params, lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "        # Training Loop\n",
        "        best_acc = 0\n",
        "        start_time_total = time.time()\n",
        "\n",
        "        print(f\"{'Epoch':<6} | {'Time':<6} | {'Train Loss':<10} | {'Train Acc':<10} | {'Val Acc':<10}\")\n",
        "        print(\"-\" * 55)\n",
        "\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            # LR Scheduler\n",
        "            if epoch in [25, 30]:\n",
        "                decay = 0.1\n",
        "                if mode == 'dgl':\n",
        "                    for opt in optimizers:\n",
        "                        for pg in opt.param_groups: pg['lr'] *= decay\n",
        "                else:\n",
        "                    for pg in optimizer.param_groups: pg['lr'] *= decay\n",
        "\n",
        "            # Train\n",
        "            if mode == 'dgl':\n",
        "                trn_loss, trn_acc = train_dgl_bp_free(train_loader, model, criterion, optimizers, device)\n",
        "            else:\n",
        "                trn_loss, trn_acc = train_standard_split(train_loader, model, criterion, optimizer, device)\n",
        "\n",
        "            # Validate\n",
        "            val_acc, val_loss = validate(val_loader, model, criterion, epoch, device, mode)\n",
        "\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            best_acc = max(best_acc, val_acc)\n",
        "\n",
        "            print(f\"{epoch:<6} | {epoch_time:.1f}s  | {trn_loss:.4f}     | {trn_acc:.2f}%     | {val_acc:.2f}%\")\n",
        "\n",
        "        total_time = time.time() - start_time_total\n",
        "        results[mode] = {'best_acc': best_acc, 'total_time': total_time}\n",
        "\n",
        "        # Cleanup to save memory for next run\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\\n\" + \"=\"*40)\n",
        "    print(\" FINAL RESULTS SUMMARY \")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{'Mode':<15} | {'Best Accuracy':<15} | {'Total Time':<15}\")\n",
        "    print(\"-\" * 50)\n",
        "    for mode in modes:\n",
        "        print(f\"{mode.upper():<15} | {results[mode]['best_acc']:.2f}%          | {results[mode]['total_time']:.1f}s\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0zuYyLET7l-",
        "outputId": "5c74fa9b-f37c-486b-e813-246b47773e51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "============================================================\n",
            " STARTING MODE: DGL \n",
            "============================================================\n",
            "Epoch  | Time   | Train Loss | Train Acc  | Val Acc   \n",
            "-------------------------------------------------------\n",
            "1      | 16.7s  | 3.4027     | 11.38%     | 14.63%\n",
            "2      | 16.1s  | 2.2569     | 13.37%     | 16.84%\n",
            "3      | 16.1s  | 1.9652     | 20.12%     | 22.03%\n",
            "4      | 16.3s  | 1.7344     | 29.99%     | 33.28%\n",
            "5      | 16.2s  | 1.4434     | 44.47%     | 41.30%\n",
            "6      | 16.1s  | 1.2599     | 52.97%     | 48.93%\n",
            "7      | 16.2s  | 1.1434     | 58.09%     | 46.43%\n",
            "8      | 16.1s  | 1.0421     | 62.51%     | 57.82%\n",
            "9      | 15.9s  | 0.9592     | 66.26%     | 51.46%\n",
            "10     | 16.1s  | 0.8736     | 69.92%     | 66.19%\n",
            "11     | 16.0s  | 0.7722     | 73.78%     | 66.32%\n",
            "12     | 15.9s  | 0.7178     | 75.59%     | 62.57%\n",
            "13     | 16.0s  | 0.6748     | 76.92%     | 70.87%\n",
            "14     | 15.9s  | 0.6335     | 78.40%     | 67.25%\n",
            "15     | 16.2s  | 0.6166     | 79.03%     | 63.52%\n",
            "16     | 15.9s  | 0.5971     | 79.72%     | 70.97%\n",
            "17     | 16.0s  | 0.5898     | 80.02%     | 60.98%\n",
            "18     | 16.0s  | 0.5605     | 80.89%     | 70.03%\n",
            "19     | 16.1s  | 0.5575     | 81.13%     | 68.92%\n",
            "20     | 15.8s  | 0.5425     | 81.70%     | 72.60%\n",
            "21     | 15.8s  | 0.5285     | 82.09%     | 69.04%\n",
            "22     | 15.9s  | 0.5142     | 82.67%     | 71.00%\n",
            "23     | 15.8s  | 0.5024     | 82.94%     | 60.95%\n",
            "24     | 16.1s  | 0.4937     | 83.37%     | 80.19%\n",
            "25     | 15.8s  | 0.3492     | 88.35%     | 87.35%\n",
            "26     | 16.0s  | 0.3041     | 89.72%     | 87.43%\n",
            "27     | 15.9s  | 0.2890     | 90.17%     | 88.08%\n",
            "28     | 15.9s  | 0.2794     | 90.62%     | 87.57%\n",
            "29     | 16.0s  | 0.2683     | 90.82%     | 88.30%\n",
            "30     | 16.2s  | 0.2483     | 91.50%     | 88.88%\n",
            "31     | 15.8s  | 0.2456     | 91.57%     | 88.99%\n",
            "32     | 15.9s  | 0.2474     | 91.46%     | 89.03%\n",
            "33     | 15.8s  | 0.2405     | 91.78%     | 88.91%\n",
            "34     | 16.0s  | 0.2400     | 91.78%     | 89.21%\n",
            "35     | 16.0s  | 0.2363     | 91.88%     | 89.02%\n",
            "36     | 15.8s  | 0.2353     | 91.94%     | 89.06%\n",
            "37     | 15.8s  | 0.2372     | 91.83%     | 89.08%\n",
            "38     | 15.9s  | 0.2366     | 91.74%     | 88.92%\n",
            "39     | 16.0s  | 0.2353     | 91.92%     | 89.01%\n",
            "40     | 15.9s  | 0.2349     | 91.89%     | 89.03%\n",
            "\n",
            "============================================================\n",
            " STARTING MODE: STANDARD \n",
            "============================================================\n",
            "Epoch  | Time   | Train Loss | Train Acc  | Val Acc   \n",
            "-------------------------------------------------------\n",
            "1      | 14.8s  | 2.6617     | 9.94%     | 10.00%\n",
            "2      | 14.7s  | 2.3036     | 9.76%     | 10.00%\n",
            "3      | 14.8s  | 2.3036     | 9.98%     | 10.00%\n",
            "4      | 14.9s  | 2.3036     | 9.75%     | 10.00%\n",
            "5      | 15.0s  | 2.3035     | 9.76%     | 10.00%\n",
            "6      | 15.1s  | 2.3034     | 10.03%     | 10.00%\n",
            "7      | 15.0s  | 2.3035     | 9.84%     | 10.00%\n",
            "8      | 15.0s  | 2.3035     | 9.98%     | 9.90%\n",
            "9      | 14.8s  | 2.2988     | 10.27%     | 11.25%\n",
            "10     | 14.9s  | 2.1026     | 18.23%     | 29.05%\n",
            "11     | 15.2s  | 1.6824     | 35.73%     | 42.71%\n",
            "12     | 15.1s  | 1.3010     | 52.70%     | 56.22%\n",
            "13     | 14.9s  | 0.9868     | 65.23%     | 50.36%\n",
            "14     | 15.0s  | 0.8278     | 71.23%     | 68.31%\n",
            "15     | 14.8s  | 0.7237     | 74.92%     | 70.11%\n",
            "16     | 14.8s  | 0.6526     | 77.46%     | 74.85%\n",
            "17     | 14.9s  | 0.6093     | 78.92%     | 70.57%\n",
            "18     | 15.0s  | 0.5690     | 80.43%     | 70.61%\n",
            "19     | 14.6s  | 0.5398     | 81.61%     | 78.06%\n",
            "20     | 14.9s  | 0.5164     | 82.16%     | 76.26%\n",
            "21     | 14.9s  | 0.4925     | 82.94%     | 72.81%\n",
            "22     | 14.9s  | 0.4790     | 83.67%     | 78.40%\n",
            "23     | 14.8s  | 0.4553     | 84.55%     | 77.77%\n",
            "24     | 14.9s  | 0.4450     | 84.56%     | 74.98%\n",
            "25     | 15.0s  | 0.3070     | 89.55%     | 88.42%\n",
            "26     | 15.2s  | 0.2628     | 91.07%     | 88.65%\n",
            "27     | 14.9s  | 0.2426     | 91.81%     | 88.97%\n",
            "28     | 14.8s  | 0.2296     | 92.06%     | 89.42%\n",
            "29     | 15.1s  | 0.2194     | 92.41%     | 89.23%\n",
            "30     | 14.9s  | 0.1929     | 93.36%     | 90.08%\n",
            "31     | 15.0s  | 0.1890     | 93.51%     | 90.20%\n",
            "32     | 14.9s  | 0.1853     | 93.65%     | 90.22%\n",
            "33     | 14.8s  | 0.1848     | 93.77%     | 90.22%\n",
            "34     | 14.9s  | 0.1810     | 93.84%     | 90.25%\n",
            "35     | 14.9s  | 0.1794     | 93.85%     | 90.05%\n",
            "36     | 15.2s  | 0.1800     | 93.85%     | 90.14%\n",
            "37     | 15.1s  | 0.1798     | 93.84%     | 90.12%\n",
            "38     | 15.2s  | 0.1744     | 94.01%     | 90.16%\n",
            "39     | 15.3s  | 0.1742     | 94.03%     | 90.34%\n",
            "40     | 15.3s  | 0.1732     | 94.09%     | 90.38%\n",
            "\n",
            "\n",
            "========================================\n",
            " FINAL RESULTS SUMMARY \n",
            "========================================\n",
            "Mode            | Best Accuracy   | Total Time     \n",
            "--------------------------------------------------\n",
            "DGL             | 89.21%          | 639.6s\n",
            "STANDARD        | 90.38%          | 598.4s\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ANNaI9IYiBq"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}